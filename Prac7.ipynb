{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffaf0bbc-89b0-423e-b36e-f9c4dd8a619b",
   "metadata": {},
   "source": [
    "Tokenization : Tokenization is the process of splitting a text document into individual words or tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb7234-1271-46b5-961b-60995fdf8b6e",
   "metadata": {},
   "source": [
    "POS Tagging: POS (Part-of-Speech) tagging is the process of assigning grammatical information (like noun, verb, adjective, etc.) to each word in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f0f50-2849-4944-92af-422d1d320f94",
   "metadata": {},
   "source": [
    "Stop words : Stop words are commonly used words (like \"the\", \"is\", \"and\", etc.) that are often removed from text data because they don't carry significant meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b4665-9152-41aa-a9ad-698841bd6579",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization : Stemming is a simpler process that chops off suffixes,while lemmatization considers the context and reduces words to their dictionary form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ac838-7bde-4f06-84aa-f6955151fd92",
   "metadata": {},
   "source": [
    "Term Frequency (TF): TF measures the frequency of a term in a document. It is calculated as the ratio of the count of a term to the total number of terms in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9e646-db1a-4912-9102-5c9c19a3ca31",
   "metadata": {},
   "source": [
    "Inverse Document Frequency (IDF): IDF measures the importance of a term across multiple documents. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a2c58f-d9fb-4ce2-ad41-6efba34f267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\samruddhi\\appdata\\roaming\\python\\python311\\site-packages (3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b941c6-2b4f-4537-b357-303bf8191b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\samruddhi\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\samruddhi\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\samruddhi\\appdata\\roaming\\python\\python311\\site-packages (from python-docx) (4.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db72fd1-0277-4a05-a0f4-abb73e059b11",
   "metadata": {},
   "source": [
    "## Extracting an sample documnent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e113a4ff-d2f8-4f51-a29d-c833464052e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt' , 'r') as file:\n",
    "    sample_document = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9834e9f1-61f7-418f-acf3-62b01be41e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tokenization is the process of splitting a text document into individual words or tokens. \\nIt is an important step in natural language processing. \\nPOS tagging assigns grammatical information to each word in a sentence. \\nStop words are commonly used words that are often removed from text data because they don't carry significant meaning.\\nStemming and lemmatization are techniques used to reduce words to their base or root form.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752a5ba-c0d3-440a-b882-91047ac541af",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d527562-215e-4338-b001-6d87edd788d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfde4e10-a257-4912-bfd3-49a8727a2791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Samruddhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f760e038-8f25-4ca9-a1e6-b0495240f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(sample_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7bafe9-3be1-49f4-89c9-41eb71e17116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'splitting',\n",
       " 'a',\n",
       " 'text',\n",
       " 'document',\n",
       " 'into',\n",
       " 'individual',\n",
       " 'words',\n",
       " 'or',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'an',\n",
       " 'important',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'POS',\n",
       " 'tagging',\n",
       " 'assigns',\n",
       " 'grammatical',\n",
       " 'information',\n",
       " 'to',\n",
       " 'each',\n",
       " 'word',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'Stop',\n",
       " 'words',\n",
       " 'are',\n",
       " 'commonly',\n",
       " 'used',\n",
       " 'words',\n",
       " 'that',\n",
       " 'are',\n",
       " 'often',\n",
       " 'removed',\n",
       " 'from',\n",
       " 'text',\n",
       " 'data',\n",
       " 'because',\n",
       " 'they',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'carry',\n",
       " 'significant',\n",
       " 'meaning',\n",
       " '.',\n",
       " 'Stemming',\n",
       " 'and',\n",
       " 'lemmatization',\n",
       " 'are',\n",
       " 'techniques',\n",
       " 'used',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'words',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'or',\n",
       " 'root',\n",
       " 'form',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38693771-61a8-4cc8-827f-aa9c31467274",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a3798b-753b-4a12-8ed5-df97cc4d6836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Samruddhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f4cad2d-6ffb-4646-8109-5c1fdff9aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75241e9-2ae2-47d5-a503-a57074b83a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tokenization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('splitting', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('document', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('individual', 'JJ'),\n",
       " ('words', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('tokens', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('important', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('.', '.'),\n",
       " ('POS', 'NNP'),\n",
       " ('tagging', 'VBG'),\n",
       " ('assigns', 'RB'),\n",
       " ('grammatical', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('each', 'DT'),\n",
       " ('word', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Stop', 'VB'),\n",
       " ('words', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('commonly', 'RB'),\n",
       " ('used', 'VBN'),\n",
       " ('words', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('often', 'RB'),\n",
       " ('removed', 'VBN'),\n",
       " ('from', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('because', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('carry', 'VB'),\n",
       " ('significant', 'JJ'),\n",
       " ('meaning', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Stemming', 'VBG'),\n",
       " ('and', 'CC'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('are', 'VBP'),\n",
       " ('techniques', 'NNS'),\n",
       " ('used', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('reduce', 'VB'),\n",
       " ('words', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('their', 'PRP$'),\n",
       " ('base', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('root', 'NN'),\n",
       " ('form', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26855ec8-a1b5-4269-b4fc-0ee54787a523",
   "metadata": {},
   "source": [
    "## Stop Words Removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d4e631-f2fb-4e49-9eef-c7b2bb779d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb6f3436-287f-407b-9cc1-c776a7cdced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samruddhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20aa76d-6494-4997-abc9-2f3959f22e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "103d62bf-6293-4914-8c80-35d44fb31517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bf0dea0-5a3f-4cad-bb55-f1a53b7d8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83fcc607-55b4-4494-a21a-aabf3c7221a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'process',\n",
       " 'splitting',\n",
       " 'text',\n",
       " 'document',\n",
       " 'individual',\n",
       " 'words',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'important',\n",
       " 'step',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'POS',\n",
       " 'tagging',\n",
       " 'assigns',\n",
       " 'grammatical',\n",
       " 'information',\n",
       " 'word',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'Stop',\n",
       " 'words',\n",
       " 'commonly',\n",
       " 'used',\n",
       " 'words',\n",
       " 'often',\n",
       " 'removed',\n",
       " 'text',\n",
       " 'data',\n",
       " \"n't\",\n",
       " 'carry',\n",
       " 'significant',\n",
       " 'meaning',\n",
       " '.',\n",
       " 'Stemming',\n",
       " 'lemmatization',\n",
       " 'techniques',\n",
       " 'used',\n",
       " 'reduce',\n",
       " 'words',\n",
       " 'base',\n",
       " 'root',\n",
       " 'form',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab4a97-0d16-427a-8593-004b3819f92e",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdd119f-808d-4cdc-8ea4-b4c046a8dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Samruddhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb302f6-29ac-4c0c-979c-2cfbd05fb254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['token', 'process', 'split', 'text', 'document', 'individu', 'word', 'token', '.', 'import', 'step', 'natur', 'languag', 'process', '.', 'po', 'tag', 'assign', 'grammat', 'inform', 'word', 'sentenc', '.', 'stop', 'word', 'commonli', 'use', 'word', 'often', 'remov', 'text', 'data', \"n't\", 'carri', 'signific', 'mean', '.', 'stem', 'lemmat', 'techniqu', 'use', 'reduc', 'word', 'base', 'root', 'form', '.']\n",
      "Lemmatized Tokens: ['Tokenization', 'process', 'splitting', 'text', 'document', 'individual', 'word', 'token', '.', 'important', 'step', 'natural', 'language', 'processing', '.', 'POS', 'tagging', 'assigns', 'grammatical', 'information', 'word', 'sentence', '.', 'Stop', 'word', 'commonly', 'used', 'word', 'often', 'removed', 'text', 'data', \"n't\", 'carry', 'significant', 'meaning', '.', 'Stemming', 'lemmatization', 'technique', 'used', 'reduce', 'word', 'base', 'root', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c969d13b-8866-47ad-9d65-c19fc1b4cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "669b0d31-b07e-4548-a05f-35f5f0a73a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n",
      "[[0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
      "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
      "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
      "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
      "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
      "  0.26490647 0.13245324 0.13245324 0.26490647 0.13245324 0.52981294]]\n",
      "\n",
      "Feature Names:\n",
      "['assigns' 'base' 'carry' 'commonly' 'data' 'document' 'form'\n",
      " 'grammatical' 'important' 'individual' 'information' 'language'\n",
      " 'lemmatization' 'meaning' 'natural' 'often' 'pos' 'process' 'processing'\n",
      " 'reduce' 'removed' 'root' 'sentence' 'significant' 'splitting' 'stemming'\n",
      " 'step' 'stop' 'tagging' 'techniques' 'text' 'tokenization' 'tokens'\n",
      " 'used' 'word' 'words']\n"
     ]
    }
   ],
   "source": [
    "filtered_document = ' '.join(filtered_tokens)\n",
    "\n",
    "# Calculate TF-IDF representation\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([filtered_document])\n",
    "\n",
    "# Print TF-IDF representation\n",
    "print(\"TF-IDF Representation:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad562974-96bf-4e1a-978e-112134c5800d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
